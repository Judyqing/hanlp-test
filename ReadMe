
参考https://github.com/hankcs/HanLP

测试第一步：
1. 分词测试一共有四个：HighSpeed ,Standard, NLP , IndexTokenizer ;
2. 比较符合常规使用的是：HighSpeed ,Standard, NLP，其中HighSpeed默认不支持添加自定义词典；
   Standard，NLP默认支持添加自定义词典，通过CustomDictionary添加；
3. 分词后，可以通过CoreStopWordDictionary.apply(terms)来对分词结果去除停用词；

测试第二步：
1. 引入Spark相关类库，测试Spark相关TF-IDF,Word2Vec等方法；


测试第三步：
1. 引入中文语料：http://www.threedweb.cn/thread-1288-1-1.html, 原语料库中“环境类”数据中有一个文件是n99.txt 改为5299.txt
2.



注意事项：
1. 如果需要使用spark-submit的方式运行，则需要把hanlp的jar包放入到maven打包的jar包中
（如果maven可以直接把相关class打包到jar中，那就需要这样做了）,参考pom文件即可；